{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test2/transforms_test.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1c8d7610ccf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'blender'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m           ('test2/', 'test',\n\u001b[0;32m---> 26\u001b[0;31m            img_wh=img_wh)\n\u001b[0m",
      "\u001b[0;32m~/nerf_holo/datasets/blender.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, split, img_wh)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhite_back\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nerf_holo/datasets/blender.py\u001b[0m in \u001b[0;36mread_meta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         with open(os.path.join(self.root_dir,\n\u001b[0;32m---> 24\u001b[0;31m                                f\"transforms_{self.split}.json\"), 'r') as f:\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test2/transforms_test.json'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils import *\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from models.rendering import *\n",
    "from models.nerf import *\n",
    "\n",
    "import metrics\n",
    "\n",
    "from datasets import dataset_dict\n",
    "from datasets.llff import *\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "img_wh = (800, 800)\n",
    "\n",
    "dataset = dataset_dict['llff'] \\\n",
    "          ('test2/', 'test_train', spheric_poses=True,\n",
    "           img_wh=img_wh)\n",
    "\n",
    "\n",
    "# dataset = dataset_dict['blender'] \\\n",
    "#           ('test2/', 'test',\n",
    "#            img_wh=img_wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_xyz = Embedding(3, 10)\n",
    "embedding_dir = Embedding(3, 4)\n",
    "\n",
    "nerf_coarse = NeRF()\n",
    "nerf_fine = NeRF()\n",
    "\n",
    "# ckpt_path = 'ckpts/exp3/epoch=5.ckpt'\n",
    "ckpt_path = 'ckpts/exp2/epoch=29.ckpt'\n",
    "# ckpt_path = 'ckpts/exp/epoch=29.ckpt'\n",
    "# ckpt_path = 'ckpts/silica3_3/epoch=29.ckpt'\n",
    "\n",
    "load_ckpt(nerf_coarse, ckpt_path, model_name='nerf_coarse')\n",
    "load_ckpt(nerf_fine, ckpt_path, model_name='nerf_fine')\n",
    "\n",
    "nerf_coarse.cuda().eval()\n",
    "nerf_fine.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [nerf_coarse, nerf_fine]\n",
    "embeddings = [embedding_xyz, embedding_dir]\n",
    "\n",
    "N_samples = 64\n",
    "N_importance = 64\n",
    "use_disp = False\n",
    "chunk = 1024*32*4\n",
    "\n",
    "@torch.no_grad()\n",
    "def f(rays):\n",
    "    \"\"\"Do batched inference on rays using chunk.\"\"\"\n",
    "    B = rays.shape[0]\n",
    "    results = defaultdict(list)\n",
    "    for i in range(0, B, chunk):\n",
    "        rendered_ray_chunks = \\\n",
    "            render_rays(models,\n",
    "                        embeddings,\n",
    "                        rays[i:i+chunk],\n",
    "                        N_samples,\n",
    "                        use_disp,\n",
    "                        0,\n",
    "                        0,\n",
    "                        N_importance,\n",
    "                        chunk,\n",
    "                        dataset.white_back,\n",
    "                        test_time=True)\n",
    "\n",
    "        for k, v in rendered_ray_chunks.items():\n",
    "            results[k] += [v]\n",
    "\n",
    "    for k, v in results.items():\n",
    "        results[k] = torch.cat(v, 0)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]\n",
    "rays = sample['rays'].cuda()\n",
    "\n",
    "t = time.time()\n",
    "results = f(rays)\n",
    "torch.cuda.synchronize()\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gt = sample['rgbs'].view(img_wh[1], img_wh[0], 3)\n",
    "img_pred = results['rgb_fine'].view(img_wh[1], img_wh[0], 3).cpu().numpy()\n",
    "alpha_pred = results['opacity_fine'].view(img_wh[1], img_wh[0]).cpu().numpy()\n",
    "depth_pred = results['depth_fine'].view(img_wh[1], img_wh[0])\n",
    "\n",
    "print('PSNR', metrics.psnr(img_gt, img_pred).item())\n",
    "\n",
    "plt.subplots(figsize=(15, 8))\n",
    "plt.tight_layout()\n",
    "plt.subplot(221)\n",
    "plt.title('GT')\n",
    "plt.imshow(img_gt)\n",
    "plt.subplot(222)\n",
    "plt.title('pred')\n",
    "plt.imshow(img_pred)\n",
    "plt.subplot(223)\n",
    "plt.title('depth')\n",
    "plt.imshow(visualize_depth(depth_pred).permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf_pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fcac3b0ee37ef2aa4c453a34ad698721dd94536c5e8cd07bec957a1356b9861"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
